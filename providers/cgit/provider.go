package cgit

import (
	"io"
	"sync"
	"time"

	"github.com/src-d/rovers/core"
	"github.com/src-d/rovers/utils"
	"github.com/src-d/rovers/utils/websearch"
	"github.com/src-d/rovers/utils/websearch/bing"

	"github.com/jpillora/backoff"
	"github.com/sourcegraph/go-vcsurl"
	repositoryModel "gop.kg/src-d/domain@v6/models/repository"
	"gopkg.in/inconshreveable/log15.v2"
	"gopkg.in/mgo.v2"
	"gopkg.in/mgo.v2/bson"
)

const (
	searchQuery = `"powered by cgit"||"generated by cgit"||"Commits per author per week"`

	cgitProviderName     = "cgit"
	repositoryCollection = "repositories"
	cgitUrlsCollection   = "cgit_urls"

	cgitURLField    = "cgiturl"
	repositoryField = "url"
	dateField       = "date"

	maxDurationToRetry = 16 * time.Second
	minDurationToRetry = 1 * time.Second
)

type repository struct {
	CgitUrl string
	URL     string
	Html    string
}

type url struct {
	CgitUrl string
	Date    time.Time
}

type provider struct {
	repositoriesColl    *mgo.Collection
	urlsCollection      *mgo.Collection
	scrapers            []*scraper
	searcher            websearch.Searcher
	backoff             *backoff.Backoff
	currentScraperIndex int
	mutex               *sync.Mutex
	lastPage            *page
}

func getBackoff() *backoff.Backoff {
	return &backoff.Backoff{
		Jitter: true,
		Factor: 2,
		Max:    maxDurationToRetry,
		Min:    minDurationToRetry,
	}
}

func NewProvider(bingKey string, database string) core.RepoProvider {
	p := &provider{
		repositoriesColl: initRepositoriesCollection(database),
		urlsCollection:   initializeCgitUrlsCollection(database),
		scrapers:         []*scraper{},
		searcher:         bing.New(bingKey),
		backoff:          getBackoff(),
		mutex:            &sync.Mutex{},
	}

	return p
}

func initializeCgitUrlsCollection(database string) *mgo.Collection {
	cgitUrlsColl := core.NewClient(database).Collection(cgitUrlsCollection)
	index := mgo.Index{
		Key:      []string{cgitURLField},
		Unique:   true,
		DropDups: true,
	}
	cgitUrlsColl.EnsureIndex(index)
	cgitUrlsColl.EnsureIndexKey(dateField)

	return cgitUrlsColl
}

func initRepositoriesCollection(database string) *mgo.Collection {
	cgitColl := core.NewClient(database).Collection(repositoryCollection)
	index := mgo.Index{
		Key: []string{"$text:" + cgitURLField, "$text:" + repositoryField},
	}
	cgitColl.EnsureIndex(index)

	return cgitColl
}

func (cp *provider) setCheckpoint(cgitUrl string, cgitPage *page) error {
	log15.Debug("adding new checkpoint url", "cgit URL", cgitUrl, "repository", cgitPage.RepositoryURL)

	return cp.repositoriesColl.Insert(
		&repository{
			CgitUrl: cgitUrl,
			URL:     cgitPage.RepositoryURL,
			Html:    cgitPage.Html,
		})
}

func (cp *provider) alreadyProcessed(cgitUrl string, cgitPage *page) (bool, error) {
	c, err := cp.repositoriesColl.Find(
		bson.M{
			cgitURLField:    cgitUrl,
			repositoryField: cgitPage.RepositoryURL,
		}).Count()

	return c > 0, err
}

func (cp *provider) getAllCgitUrlsAlreadyProcessed() ([]string, error) {
	cgitUrls := []*url{}
	err := cp.urlsCollection.Find(nil).All(&cgitUrls)
	result := []string{}
	for _, cu := range cgitUrls {
		result = append(result, cu.CgitUrl)
	}

	return result, err
}

func (cp *provider) saveNewCgitUrls(urls []string) error {
	for _, u := range urls {
		err := cp.urlsCollection.Insert(&url{CgitUrl: u, Date: time.Now()})
		switch {
		case err == nil:
			log15.Debug("New inserted cgit URL", "url", u)
		case mgo.IsDup(err):
			log15.Debug("Duplicated cgit URL", "url", u)
		default:
			return err
		}
	}

	return nil
}

func (cp *provider) fillScrapers() {
	cgitUrlsSet := map[string]struct{}{}
	alreadyProcessedCgitUrls, err := cp.getAllCgitUrlsAlreadyProcessed()
	if err != nil {
		log15.Error("error getting cgit urls from database", "error", err)
	}

	possibleCgitUrls, err := cp.searcher.Search(searchQuery)
	if err != nil {
		log15.Error("error getting cgit urls from Searcher", "error", err)
	}

	mainCgitUrls := getAllMainCgitUrls(utils.URLsToStrings(possibleCgitUrls...))
	if err := cp.saveNewCgitUrls(mainCgitUrls); err != nil {
		log15.Error("Error saving new cgit urls", "missed cgit urls", mainCgitUrls, "error", err)
	}

	cp.joinUnique(cgitUrlsSet, mainCgitUrls, alreadyProcessedCgitUrls)
	for u := range cgitUrlsSet {
		log15.Info("adding new Scraper", "cgit URL", u)
		cp.scrapers = append(cp.scrapers, newScraper(u))
	}
}

func (cp *provider) joinUnique(set map[string]struct{}, slices ...[]string) {
	for _, slice := range slices {
		for _, e := range slice {
			set[e] = struct{}{}
		}
	}
}

func (cp *provider) Next() (*repositoryModel.Raw, error) {
	cp.mutex.Lock()
	defer cp.mutex.Unlock()
	if cp.lastPage != nil {
		log15.Warn("some error happens when try to call Ack(), returning the last repository again",
			"repository", cp.lastPage.RepositoryURL)

		return cp.repositoryRaw(cp.lastPage.RepositoryURL), nil
	}

	if cp.isFirst() {
		cp.fillScrapers()
		if len(cp.scrapers) == 0 {
			log15.Warn("no scrapers found, sending an EOF because we have no data")
			return nil, io.EOF
		}
	}

	for {
		currentScraper := cp.scrapers[cp.currentScraperIndex]
		cgitUrl := currentScraper.URL
		repoData, err := currentScraper.Next()
		switch {
		case err == io.EOF:
			cp.nextScraper()
			if len(cp.scrapers) <= cp.currentScraperIndex {
				log15.Debug("all cgitUrls processed, ending provider iterator",
					"current index", cp.currentScraperIndex)
				cp.reset()
				return nil, io.EOF
			}
		case err != nil:
			log15.Error("error on scraper.next", "cgit URL", currentScraper.URL, "error", err)
			cp.handleRetries()
			return nil, err
		case err == nil:
			cp.backoff.Reset()
			processed, err := cp.alreadyProcessed(cgitUrl, repoData)
			if err != nil {
				return nil, err
			}

			if processed {
				log15.Debug("repository already processed", "cgit URL", cgitUrl, "url", repoData.RepositoryURL)
			} else {
				cp.lastPage = repoData
				return cp.repositoryRaw(repoData.RepositoryURL), nil
			}
		}
	}
}

func (cp *provider) handleRetries() {
	tts := cp.backoff.Duration()
	log15.Info("sleeping before next scraper request",
		"retries", cp.backoff.Attempt(), "time to sleep", tts)
	time.Sleep(tts)

	if tts >= maxDurationToRetry {
		log15.Warn("scraper request failed too many times. Skipping to the next scraper",
			"retries", cp.backoff.Attempt())
		cp.nextScraper()
	}
}

func (*provider) repositoryRaw(repoUrl string) *repositoryModel.Raw {
	return &repositoryModel.Raw{
		Status:   repositoryModel.Initial,
		Provider: cgitProviderName,
		URL:      repoUrl,
		VCS:      vcsurl.Git,
	}
}

func (cp *provider) nextScraper() {
	cp.backoff.Reset()
	cp.currentScraperIndex++
}

func (cp *provider) isFirst() bool {
	return len(cp.scrapers) == 0
}

func (cp *provider) reset() {
	cp.scrapers = []*scraper{}
	cp.currentScraperIndex = 0
}

func (cp *provider) Ack(err error) error {
	cp.mutex.Lock()
	defer cp.mutex.Unlock()
	if err == nil && cp.lastPage != nil {
		err = cp.setCheckpoint(cp.scrapers[cp.currentScraperIndex].URL, cp.lastPage)
		if err != nil {
			return err
		} else {
			cp.lastPage = nil
		}
	}

	return nil
}

func (cp *provider) Close() error {
	return nil
}

func (cp *provider) Name() string {
	return cgitProviderName
}
